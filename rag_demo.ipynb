{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initialize-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis.sanmartin\\AppData\\Local\\Temp\\ipykernel_6628\\1485541037.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n",
      "C:\\Users\\luis.sanmartin\\AppData\\Local\\Temp\\ipykernel_6628\\1485541037.py:3: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
      "  llm = Ollama(\n"
     ]
    }
   ],
   "source": [
    "# Configuración del modelo\n",
    "MODEL = \"qwen2.5:7b\"\n",
    "llm = Ollama(\n",
    "    model=MODEL,\n",
    "    temperature=0.9,\n",
    "    top_p=0.9,\n",
    "    num_ctx=4096,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "# Ejemplo de generación inicial de datos\n",
    "#response = llm.invoke(\"genera una tabla con dos filas, valores variados y realistas y los siguientes campos: device_id, timestamp, bandwidth_mbps, latency_ms, packet_loss, signal_strength_dbm, cell_id, connection_type. Responde SOLO con el JSON puro, sin markdown, sin comillas triples, sin explicaciones.\")\n",
    "#display(response)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "create-embeddings-huggingface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['device_id', 'timestamp', 'bandwidth_mbps', 'latency_ms', 'packet_loss', 'signal_strength_dbm', 'cell_id', 'connection_type']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis.sanmartin\\AppData\\Local\\Temp\\ipykernel_6628\\717872230.py:50: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL)\n",
      "c:\\Users\\luis.sanmartin\\AppData\\Local\\miniconda3\\envs\\genia_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se generaron 1000 fragmentos de texto.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>bandwidth_mbps</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>packet_loss</th>\n",
       "      <th>signal_strength_dbm</th>\n",
       "      <th>cell_id</th>\n",
       "      <th>connection_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5851</td>\n",
       "      <td>2024-12-13T14:46:24.173261</td>\n",
       "      <td>536.156239</td>\n",
       "      <td>6.551220</td>\n",
       "      <td>0.037045</td>\n",
       "      <td>-101.327696</td>\n",
       "      <td>57</td>\n",
       "      <td>MIMO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4856</td>\n",
       "      <td>2024-12-13T15:46:24.173271</td>\n",
       "      <td>485.536208</td>\n",
       "      <td>9.675234</td>\n",
       "      <td>0.040759</td>\n",
       "      <td>-99.914445</td>\n",
       "      <td>66</td>\n",
       "      <td>MIMO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3914</td>\n",
       "      <td>2024-12-13T16:46:24.173273</td>\n",
       "      <td>420.810843</td>\n",
       "      <td>8.836038</td>\n",
       "      <td>0.049106</td>\n",
       "      <td>-98.675641</td>\n",
       "      <td>59</td>\n",
       "      <td>MIMO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9267</td>\n",
       "      <td>2024-12-13T17:46:24.173274</td>\n",
       "      <td>736.451439</td>\n",
       "      <td>8.656040</td>\n",
       "      <td>0.056578</td>\n",
       "      <td>-80.390779</td>\n",
       "      <td>68</td>\n",
       "      <td>Carrier Aggregation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4908</td>\n",
       "      <td>2024-12-13T18:46:24.173275</td>\n",
       "      <td>524.275720</td>\n",
       "      <td>11.614536</td>\n",
       "      <td>0.022364</td>\n",
       "      <td>-109.168636</td>\n",
       "      <td>98</td>\n",
       "      <td>MIMO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   device_id                   timestamp  bandwidth_mbps  latency_ms  \\\n",
       "0       5851  2024-12-13T14:46:24.173261      536.156239    6.551220   \n",
       "1       4856  2024-12-13T15:46:24.173271      485.536208    9.675234   \n",
       "2       3914  2024-12-13T16:46:24.173273      420.810843    8.836038   \n",
       "3       9267  2024-12-13T17:46:24.173274      736.451439    8.656040   \n",
       "4       4908  2024-12-13T18:46:24.173275      524.275720   11.614536   \n",
       "\n",
       "   packet_loss  signal_strength_dbm  cell_id      connection_type  \n",
       "0     0.037045          -101.327696       57                 MIMO  \n",
       "1     0.040759           -99.914445       66                 MIMO  \n",
       "2     0.049106           -98.675641       59                 MIMO  \n",
       "3     0.056578           -80.390779       68  Carrier Aggregation  \n",
       "4     0.022364          -109.168636       98                 MIMO  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar archivo JSON localmente (esto puede ser un archivo de ejemplo)\n",
    "json_file = \"json_dataset.json\"  # Asegúrate de que este archivo esté en la misma carpeta o proporciona la ruta completa\n",
    "\n",
    "# Leer el archivo JSON de forma asíncrona\n",
    "with open(json_file, \"r\") as f:\n",
    "    contents = f.read()\n",
    "    json_data = json.loads(contents)\n",
    "\n",
    "# Extraer registros según la estructura del JSON\n",
    "if isinstance(json_data, dict) and \"data\" in json_data:\n",
    "    # Si es la estructura de MongoDB\n",
    "    records = json_data[\"data\"]\n",
    "elif isinstance(json_data, list):\n",
    "    # Si es una lista directa de registros\n",
    "    records = json_data\n",
    "else:\n",
    "    # Si es un único registro\n",
    "    records = [json_data]\n",
    "\n",
    "# Convertir los registros a DataFrame y limpiar\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Eliminar columnas no deseadas si existen\n",
    "columns_to_drop = ['_id', 'dataset_id']\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "# Verificar columnas resultantes\n",
    "fields = df.columns.tolist()\n",
    "print(fields)\n",
    "\n",
    "# Crear documentos para embeddings (concatenar columnas en una cadena)\n",
    "documents = []\n",
    "for _, row in df.iterrows():\n",
    "    doc_text = \" \".join([f\"{col}: {val}\" for col, val in row.items()])\n",
    "    documents.append(doc_text)\n",
    "\n",
    "# Crear un splitter para dividir los textos largos en fragmentos\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Chunks más pequeños\n",
    "    chunk_overlap=50       # Menor superposición\n",
    ")\n",
    "\n",
    "# Dividir los documentos en fragmentos\n",
    "split_docs = []\n",
    "for doc in documents:\n",
    "    split_docs.extend(text_splitter.split_text(doc))\n",
    "\n",
    "# Definir el modelo de embeddings de HuggingFace\n",
    "EMBEDDINGS_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL)\n",
    "\n",
    "# Crear y guardar los embeddings en ChromaDB\n",
    "db = Chroma.from_texts(texts=split_docs, embedding=embeddings)\n",
    "\n",
    "# Verifica el número de fragmentos generados\n",
    "print(f\"Se generaron {len(split_docs)} fragmentos de texto.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "# Definir un prompt base\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    eres un experto en ciberseguridad y basándote en este dataset y sus campos:\n",
    "    {context}\n",
    "\n",
    "    Responde a la pregunta del usuario:\n",
    "    {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Configurar la cadena RAG\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "# Construir la pregunta dinámica con los campos\n",
    "fields_str = ', '.join(fields)  # Convertir la lista de columnas a una cadena\n",
    "num_samples = 3  # Número de registros a generar\n",
    "\n",
    "# Ejemplo de consulta al sistema\n",
    "question = f\"Genera una tabla con {num_samples} filas, valores random y los siguientes campos: {fields_str}. Responde SOLO con el JSON puro, sin markdown, sin comillas triples, sin explicaciones.\"\n",
    "response = rag_chain.invoke(question)\n",
    "display(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extraer y limpiar el campo 'result'\n",
    "raw_result = response['result']\n",
    "\n",
    "# 1. Separar los objetos JSON por línea y limpiar el formato\n",
    "json_objects = raw_result.split(\"}\\n{\")\n",
    "json_objects = [obj if obj.startswith(\"{\") else \"{\" + obj for obj in json_objects]\n",
    "json_objects = [obj if obj.endswith(\"}\") else obj + \"}\" for obj in json_objects]\n",
    "\n",
    "# 2. Convertir cada objeto en un diccionario\n",
    "data = [json.loads(obj) for obj in json_objects]\n",
    "\n",
    "# 3. Crear un DataFrame a partir de los datos\n",
    "df_generated = pd.DataFrame(data)\n",
    "\n",
    "# 4. Guardar el DataFrame como archivo CSV\n",
    "output_csv = \"output.csv\"\n",
    "df_generated.to_csv(output_csv, index=False)\n",
    "\n",
    "\n",
    "df_generated.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
